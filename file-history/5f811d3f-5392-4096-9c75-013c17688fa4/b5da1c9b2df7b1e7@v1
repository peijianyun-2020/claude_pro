"""
é¢„æµ‹2026å¹´1æœˆ3æ—¥å¹¿ä¸œæ—¥å‰ç°è´§ä»·æ ¼

å®Œæ•´æµç¨‹ï¼š
1. å¤„ç†åŸå§‹æ•°æ®
2. è®­ç»ƒæ—¥å‰ä»·æ ¼æ¨¡å‹
3. æ‰§è¡Œé¢„æµ‹
"""

import sys
import logging
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/prediction_20260103.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def process_raw_data():
    """å¤„ç†åŸå§‹æ—¥å‰ä»·æ ¼æ•°æ®"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤1: å¤„ç†åŸå§‹æ—¥å‰ä»·æ ¼æ•°æ®")
    logger.info("=" * 60)

    from src.data.day_ahead_data_loader import DayAheadDataLoader
    from src.data.preprocessing import DataPreprocessor

    # åŠ è½½åŸå§‹æ•°æ®
    logger.info("åŠ è½½åŸå§‹æ—¥å‰ä»·æ ¼æ•°æ®...")
    loader = DayAheadDataLoader(region='å¹¿ä¸œ')
    raw_df = loader.load_all_data()

    if raw_df is None or len(raw_df) == 0:
        logger.error("æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
        return None

    logger.info(f"æˆåŠŸåŠ è½½ {len(raw_df)} æ¡æ•°æ®è®°å½•")
    logger.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {raw_df['datetime'].min()} åˆ° {raw_df['datetime'].max()}")

    # æ•°æ®é¢„å¤„ç†
    logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    preprocessor = DataPreprocessor(region='å¹¿ä¸œ')

    # å¤„ç†ç¼ºå¤±å€¼
    df_clean = preprocessor.handle_missing_values(raw_df)
    logger.info(f"ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼Œæ•°æ®é‡: {len(df_clean)}")

    # æ·»åŠ æ—¶é—´ç‰¹å¾
    df_features = preprocessor.add_time_features(df_clean)
    logger.info(f"æ—¶é—´ç‰¹å¾æ·»åŠ å®Œæˆï¼Œç‰¹å¾æ•°: {len(df_features.columns)}")

    # æ·»åŠ æ»šåŠ¨ç‰¹å¾
    df_rolling = preprocessor.add_rolling_features(df_features)
    logger.info(f"æ»šåŠ¨ç‰¹å¾æ·»åŠ å®Œæˆï¼Œç‰¹å¾æ•°: {len(df_rolling.columns)}")

    # æ·»åŠ æ»åç‰¹å¾
    df_final = preprocessor.add_lag_features(df_rolling)
    logger.info(f"æ»åç‰¹å¾æ·»åŠ å®Œæˆï¼Œæœ€ç»ˆç‰¹å¾æ•°: {len(df_final.columns)}")

    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    output_path = Path('data/processed/day_ahead_processed.csv')
    output_path.parent.mkdir(parents=True, exist_ok=True)

    df_final.to_csv(output_path, index=False, encoding='utf-8-sig')
    logger.info(f"âœ… å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    logger.info(f"æœ€ç»ˆæ•°æ®é‡: {len(df_final)} è¡Œ Ã— {len(df_final.columns)} åˆ—")

    return df_final


def train_model(processed_data):
    """è®­ç»ƒæ—¥å‰ä»·æ ¼é¢„æµ‹æ¨¡å‹"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤2: è®­ç»ƒæ—¥å‰ä»·æ ¼é¢„æµ‹æ¨¡å‹")
    logger.info("=" * 60)

    from src.models.day_ahead_model import DayAheadXGBoostModel

    # åˆ›å»ºæ¨¡å‹
    logger.info("åˆå§‹åŒ–XGBoostæ—¥å‰ä»·æ ¼æ¨¡å‹...")
    model = DayAheadXGBoostModel(
        region='å¹¿ä¸œ',
        lookback_days=30,
        forecast_days=3
    )

    # å‡†å¤‡è®­ç»ƒæ•°æ®
    logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
    X_train, X_test, y_train, y_test = model.prepare_training_data(processed_data)

    logger.info(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    logger.info(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")

    # è®­ç»ƒæ¨¡å‹
    logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
    model.train()

    # è¯„ä¼°æ¨¡å‹
    logger.info("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    train_mape = model.evaluate(X_train, y_train)
    test_mape = model.evaluate(X_test, y_test)

    logger.info(f"è®­ç»ƒé›†MAPE: {train_mape:.2f}%")
    logger.info(f"æµ‹è¯•é›†MAPE: {test_mape:.2f}%")

    # ä¿å­˜æ¨¡å‹
    model_path = Path('models/xgb_day_ahead.pkl')
    model_path.parent.mkdir(parents=True, exist_ok=True)

    model.save(str(model_path))
    logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

    return model


def run_prediction():
    """æ‰§è¡Œé¢„æµ‹"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤3: é¢„æµ‹2026å¹´1æœˆ3æ—¥æ—¥å‰ä»·æ ¼")
    logger.info("=" * 60)

    from src.prediction.prediction_workflow import PredictionWorkflow

    # åˆ›å»ºé¢„æµ‹å·¥ä½œæµ
    logger.info("åˆå§‹åŒ–é¢„æµ‹å·¥ä½œæµ...")
    workflow = PredictionWorkflow(
        region='å¹¿ä¸œ',
        lookback_days=30,
        forecast_days=3
    )

    # è¿è¡Œé¢„æµ‹
    logger.info("æ‰§è¡Œé¢„æµ‹...")
    try:
        result = workflow.run_daily_prediction(save_results=True)

        # æå–é¢„æµ‹ç»“æœ
        if 'ensemble' in result:
            predictions = result['ensemble']
        elif 'xgboost_day' in result:
            predictions = result['xgboost_day']
        else:
            logger.error("é¢„æµ‹ç»“æœä¸­æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹æ•°æ®")
            return None

        logger.info(f"âœ… é¢„æµ‹å®Œæˆï¼å…±é¢„æµ‹ {len(predictions)} å°æ—¶")

        # è§£æé¢„æµ‹ç»“æœï¼ˆ3å¤©Ã—24å°æ—¶=72å°æ—¶ï¼‰
        logger.info("\n" + "=" * 60)
        logger.info("2026å¹´1æœˆ3æ—¥æ—¥å‰ä»·æ ¼é¢„æµ‹ç»“æœ")
        logger.info("=" * 60)

        # ç¬¬ä¸€å¤©ï¼ˆ2026-01-03ï¼‰
        day1_prices = predictions[:24]
        logger.info(f"\nğŸ“… 2026å¹´1æœˆ3æ—¥ï¼ˆç¬¬1å¤©ï¼‰:")
        logger.info(f"   å¹³å‡ä»·æ ¼: {np.mean(day1_prices):.2f} å…ƒ/MWh")
        logger.info(f"   æœ€ä½ä»·æ ¼: {np.min(day1_prices):.2f} å…ƒ/MWh (å°æ—¶ {np.argmin(day1_prices)})")
        logger.info(f"   æœ€é«˜ä»·æ ¼: {np.max(day1_prices):.2f} å…ƒ/MWh (å°æ—¶ {np.argmax(day1_prices)})")
        logger.info(f"   ä»·æ ¼æ ‡å‡†å·®: {np.std(day1_prices):.2f}")

        # è¯¦ç»†å°æ—¶ä»·æ ¼
        logger.info(f"\nâ° 24å°æ—¶ä»·æ ¼æ˜ç»†:")
        for hour in range(24):
            logger.info(f"   {hour:02d}:00 - {day1_prices[hour]:.2f} å…ƒ/MWh")

        # ç¬¬äºŒå¤©å’Œç¬¬ä¸‰å¤©é¢„æµ‹
        if len(predictions) >= 48:
            day2_prices = predictions[24:48]
            logger.info(f"\nğŸ“… 2026å¹´1æœˆ4æ—¥ï¼ˆç¬¬2å¤©ï¼‰:")
            logger.info(f"   å¹³å‡ä»·æ ¼: {np.mean(day2_prices):.2f} å…ƒ/MWh")
            logger.info(f"   æœ€ä½ä»·æ ¼: {np.min(day2_prices):.2f} å…ƒ/MWh")
            logger.info(f"   æœ€é«˜ä»·æ ¼: {np.max(day2_prices):.2f} å…ƒ/MWh")

        if len(predictions) >= 72:
            day3_prices = predictions[48:72]
            logger.info(f"\nğŸ“… 2026å¹´1æœˆ5æ—¥ï¼ˆç¬¬3å¤©ï¼‰:")
            logger.info(f"   å¹³å‡ä»·æ ¼: {np.mean(day3_prices):.2f} å…ƒ/MWh")
            logger.info(f"   æœ€ä½ä»·æ ¼: {np.min(day3_prices):.2f} å…ƒ/MWh")
            logger.info(f"   æœ€é«˜ä»·æ ¼: {np.max(day3_prices):.2f} å…ƒ/MWh")

        logger.info("\n" + "=" * 60)

        return predictions

    except Exception as e:
        logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹é¢„æµ‹2026å¹´1æœˆ3æ—¥å¹¿ä¸œæ—¥å‰ç°è´§ä»·æ ¼")
    logger.info(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        # æ­¥éª¤1: å¤„ç†åŸå§‹æ•°æ®
        processed_data = process_raw_data()
        if processed_data is None:
            logger.error("æ•°æ®å¤„ç†å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
            return

        # æ­¥éª¤2: è®­ç»ƒæ¨¡å‹
        model = train_model(processed_data)
        if model is None:
            logger.error("æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
            return

        # æ­¥éª¤3: è¿è¡Œé¢„æµ‹
        predictions = run_prediction()
        if predictions is None:
            logger.error("é¢„æµ‹å¤±è´¥")
            return

        logger.info("\nğŸ‰ é¢„æµ‹æµç¨‹å…¨éƒ¨å®Œæˆï¼")

    except Exception as e:
        logger.error(f"æµç¨‹æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
